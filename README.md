# ğŸ¤– Prompt-Based AI CLI Tool using Ollama

> A simple CLI-based application that allows users to send text prompts or input files to a local LLM (via Ollama), and receive intelligent responses. Designed to demonstrate interaction with local language models using command-line tools.

---

## ğŸ§  Overview

This tool provides a flexible interface for experimenting with LLM prompts using direct command-line input or from `.txt` files, and saving the output to a response file.

Supports:

- ğŸ’¬ Prompt input directly or from text files
- ğŸ§  Uses locally running LLM via Ollama (`llama3` by default)
- ğŸ’¾ Optional saving of output responses to files
- ğŸ› ï¸ Minimal dependencies, runs entirely offline

---

## ğŸ” Features

- ğŸ§¾ Accept prompt input via `--prompt` flag or `--file` path
- ğŸ“¥ Reads prompt text from predefined folders
- ğŸ“¤ Outputs the response to a file (optional)
- ğŸ§  Uses Ollamaâ€™s local inference engine for fast responses
- ğŸš« No API key or external calls required

---

## ğŸ› ï¸ Tech Stack

| Component   | Technology     |
|-------------|----------------|
| CLI Tool    | Python + argparse |
| LLM Engine  | Ollama (local) |
| Request API | `requests`     |

---

## ğŸ“ Input/Output Folder Usage

- All prompt `.txt` files are placed in `input_prompts/`
- Output response files will be saved in `output_responses/`

You can customize file names using the `--file` and `--output` flags.

---

## ğŸš€ How to Run

### ğŸ”¸ Option A: Provide Prompt via CLI

```bash
python day4prompt.py "What is AI?"
```
### ğŸ”¸ Option B: Read Prompt from File and Save Output

```bash
python week1.py --file input_prompts/one_shot.txt --output output_responses/response.txt
```
### ğŸ”¸ Option C: Direct Prompt with Model and Custom Output

```bash
python week1.py --prompt "Explain quantum computing" --model llama3 --output output_responses/quantum.txt
```
